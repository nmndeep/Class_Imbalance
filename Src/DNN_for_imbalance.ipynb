{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import metrics\n",
    "# Make results reproducible\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Loading the dataset\n",
    "dat = pd.read_csv('Path Of the Data')\n",
    "values = list(dat.columns.values)\n",
    "X=dat.drop(['Y_class'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naman/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49941, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dat['Y_class']\n",
    "y = pd.get_dummies(y, columns=['Y_class']) # One Hot Encoding\n",
    "\n",
    "y = np.array(y, dtype='float32')\n",
    "X = np.array(X, dtype='float32')\n",
    "\n",
    "# Shuffle Data\n",
    "indices = np.random.choice(len(X), len(X), replace=False)\n",
    "X_values = X[indices]\n",
    "y_values = y[indices]\n",
    "\n",
    "# Creating a Train and a Test Dataset\n",
    "test_size = 12000 # change accordingly\n",
    "X_test = X_values[-test_size:]\n",
    "X_tr = X_values[:-test_size]\n",
    "y_test = y_values[-test_size:]\n",
    "y_tr = y_values[:-test_size]\n",
    "y_t = np.argmax(y_tr, axis=1)\n",
    "\n",
    "# As the data is imbalance we apply respective balancing technique  \n",
    "\n",
    "sm = RandomOverSampler(ratio=1.0)\n",
    "X_train, y_train = sm.fit_sample(X_tr, y_t)\n",
    "pp = pd.get_dummies(y_train) # One Hot Encoding\n",
    "y_train = np.array(pp, dtype='float32')\n",
    "X_train = np.array(X_train, dtype='float32')\n",
    "X.shape\n",
    "\n",
    "# Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Session\n",
    "\n",
    "    # Interval / Epochs\n",
    "    interval = 100 # Change accordingly\n",
    "    epoch = 500\n",
    "\n",
    "    # Initialize placeholders\n",
    "    X_data = tf.placeholder(shape=[None,no_of_features], dtype=tf.float32)\n",
    "    y_target = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "    P=tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "    Y=tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "\n",
    "    # Input neurons : 29    \n",
    "    # Output neurons : 2\n",
    "    hidden_layer_nodes2 = 400\n",
    "    hidden_layer_nodes1 = 1200\n",
    "\n",
    "    # Create variables for Neural Network layers\n",
    "    w1 = tf.Variable(tf.random_normal(shape=[no_of_features,hidden_layer_nodes1])) # Inputs -> Hidden Layer\n",
    "    b1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes1]))   # First Bias\n",
    "    w2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes1,hidden_layer_nodes2])) # Inputs -> Hidden Layer\n",
    "    b2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes2]))  \n",
    "    w3 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes2, hidden_layer_nodes2])) # Hidden layer -> Outputs\n",
    "    b3 = tf.Variable(tf.random_normal(shape=[2]))   # Second Bias\n",
    "   \n",
    "    # Operations\n",
    "       \n",
    "    logits1 = tf.matmul(X_data, w1,name=\"LOGIT1\") + b1\n",
    "    hidden_output1=tf.nn.tanh(logits1)\n",
    "    logits2 = tf.matmul(hidden_output1, w2,name=\"LOGIT2\") + b2\n",
    "    hidden_output2=tf.nn.tanh(logits2)\n",
    "    logits3 = tf.matmul(hidden_output2, w3,name=\"LOGIT3\") + b3\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_target, logits=logits3),name='loss')\n",
    "    \n",
    "    regularizer = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) + tf.nn.l2_loss(w3)\n",
    "    loss = tf.reduce_mean(loss + 0.01 * regularizer)\n",
    "    \n",
    "    # Cost Function\n",
    "    #loss = tf.reduce_mean(-tf.reduce_sum(y_target * tf.log(final_output), axis=0))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    #train predict\n",
    "    logit1 = tf.matmul(X_train, w1,name=\"LOGIT1\") + b1\n",
    "    rel1=tf.nn.tanh(logit1)\n",
    "    logit2 = tf.matmul(rel1, w2,name=\"LOGIT2\") + b2\n",
    "    rel2=tf.nn.tanh(logit2)\n",
    "    logit3 = tf.matmul(rel2, w3,name=\"LOGIT2\") + b3\n",
    "    train_prediction = tf.nn.softmax(logit3)\n",
    "    confusion_matrix_tp= tf.confusion_matrix(tf.argmax(y_train,1),tf.argmax(train_prediction,1),2)\n",
    "\n",
    "    #Test\n",
    "    logit1 = tf.matmul(X_test, w1,name=\"LOGIT1\") + b1\n",
    "    rel1=tf.nn.tanh(logit1)\n",
    "    logit2 = tf.matmul(rel1, w2,name=\"LOGIT2\") + b2\n",
    "    rel2=tf.nn.tanh(logit2)\n",
    "    logit3 = tf.matmul(rel2, w3,name=\"LOGIT2\") + b3\n",
    "    test_prediction = tf.nn.softmax(logit3)\n",
    "\n",
    "    confusion_matrix_tf = tf.confusion_matrix(tf.argmax(y_test,1),tf.argmax(test_prediction,1),2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model... Step : 0\n",
      "Epoch 100 | Loss: 802.57\n",
      "Epoch 200 | Loss: 695.4319\n",
      "Epoch 300 | Loss: 602.18585\n",
      "Epoch 400 | Loss: 521.15594\n",
      "Epoch 500 | Loss: 450.8693\n",
      "Training the model... Step : 1\n",
      "Epoch 100 | Loss: 389.891\n",
      "Epoch 200 | Loss: 336.82483\n",
      "Epoch 300 | Loss: 290.7504\n",
      "Epoch 400 | Loss: 250.8034\n",
      "Epoch 500 | Loss: 216.09802\n",
      "Training the model... Step : 2\n",
      "Epoch 100 | Loss: 186.0571\n",
      "Epoch 200 | Loss: 160.10742\n",
      "Epoch 300 | Loss: 137.59148\n",
      "Epoch 400 | Loss: 118.05076\n",
      "Epoch 500 | Loss: 101.20099\n",
      "Training the model... Step : 3\n",
      "Epoch 100 | Loss: 86.56081\n",
      "Epoch 200 | Loss: 73.98009\n",
      "Epoch 300 | Loss: 63.101093\n",
      "Epoch 400 | Loss: 53.740368\n",
      "Epoch 500 | Loss: 45.67689\n",
      "Training the model... Step : 4\n",
      "Epoch 100 | Loss: 38.79979\n",
      "Epoch 200 | Loss: 32.87468\n",
      "Epoch 300 | Loss: 27.834486\n",
      "Epoch 400 | Loss: 23.537336\n",
      "Epoch 500 | Loss: 19.930746\n",
      "Training the model... Step : 5\n",
      "Epoch 100 | Loss: 16.772242\n",
      "Epoch 200 | Loss: 14.126692\n",
      "Epoch 300 | Loss: 11.90414\n",
      "Epoch 400 | Loss: 10.036345\n",
      "Epoch 500 | Loss: 8.454046\n",
      "Training the model... Step : 6\n",
      "Epoch 100 | Loss: 7.1366034\n",
      "Epoch 200 | Loss: 6.0244594\n",
      "Epoch 300 | Loss: 5.1012907\n",
      "Epoch 400 | Loss: 4.34178\n",
      "Epoch 500 | Loss: 3.6933546\n",
      "Training the model... Step : 7\n",
      "Epoch 100 | Loss: 3.1645024\n",
      "Epoch 200 | Loss: 2.7314882\n",
      "Epoch 300 | Loss: 2.3678713\n",
      "Epoch 400 | Loss: 2.0701423\n",
      "Epoch 500 | Loss: 1.8269222\n",
      "Training the model... Step : 8\n",
      "Epoch 100 | Loss: 1.6265013\n",
      "Epoch 200 | Loss: 1.5064586\n",
      "Epoch 300 | Loss: 1.3257107\n",
      "Epoch 400 | Loss: 1.2162342\n",
      "Epoch 500 | Loss: 1.1247346\n",
      "Training the model... Step : 9\n",
      "Epoch 100 | Loss: 1.0513321\n",
      "Epoch 200 | Loss: 0.9923271\n",
      "Epoch 300 | Loss: 0.94287544\n",
      "Epoch 400 | Loss: 0.90467966\n",
      "Epoch 500 | Loss: 0.869973\n",
      "Training the model... Step : 10\n",
      "Epoch 100 | Loss: 0.8441605\n",
      "Epoch 200 | Loss: 0.82192695\n",
      "Epoch 300 | Loss: 0.8113177\n",
      "Epoch 400 | Loss: 0.7868315\n",
      "Epoch 500 | Loss: 0.7737776\n",
      "Training the model... Step : 11\n",
      "Epoch 100 | Loss: 0.76377094\n",
      "Epoch 200 | Loss: 0.7545718\n",
      "Epoch 300 | Loss: 0.7498987\n",
      "Epoch 400 | Loss: 0.7420213\n",
      "Epoch 500 | Loss: 0.73662794\n",
      "Training the model... Step : 12\n",
      "Epoch 100 | Loss: 0.7316399\n",
      "Epoch 200 | Loss: 0.72777075\n",
      "Epoch 300 | Loss: 0.7241564\n",
      "Epoch 400 | Loss: 0.72218055\n",
      "Epoch 500 | Loss: 0.7183844\n",
      "Training the model... Step : 13\n",
      "Epoch 100 | Loss: 0.71470785\n",
      "Epoch 200 | Loss: 0.7127198\n",
      "Epoch 300 | Loss: 0.7111363\n",
      "Epoch 400 | Loss: 0.7084085\n",
      "Epoch 500 | Loss: 0.70869887\n",
      "Training the model... Step : 14\n",
      "Epoch 100 | Loss: 0.7058994\n",
      "Epoch 200 | Loss: 0.70446736\n",
      "Epoch 300 | Loss: 0.7027385\n",
      "Epoch 400 | Loss: 0.70250577\n",
      "Epoch 500 | Loss: 0.70120114\n",
      "Training the model... Step : 15\n",
      "Epoch 100 | Loss: 0.69974065\n",
      "Epoch 200 | Loss: 0.6983282\n",
      "Epoch 300 | Loss: 0.6997402\n",
      "Epoch 400 | Loss: 0.69696957\n",
      "Epoch 500 | Loss: 0.6958729\n",
      "Training the model... Step : 16\n",
      "Epoch 100 | Loss: 0.694925\n",
      "Epoch 200 | Loss: 0.69800496\n",
      "Epoch 300 | Loss: 0.69444394\n",
      "Epoch 400 | Loss: 0.69280916\n",
      "Epoch 500 | Loss: 0.6931013\n",
      "Training the model... Step : 17\n",
      "Epoch 100 | Loss: 0.69192594\n",
      "Epoch 200 | Loss: 0.6913858\n",
      "Epoch 300 | Loss: 0.6910221\n",
      "Epoch 400 | Loss: 0.6909906\n",
      "Epoch 500 | Loss: 0.6928394\n",
      "Training the model... Step : 18\n",
      "Epoch 100 | Loss: 0.6875306\n",
      "Epoch 200 | Loss: 0.68524516\n",
      "Epoch 300 | Loss: 0.6841327\n",
      "Epoch 400 | Loss: 0.68500745\n",
      "Epoch 500 | Loss: 0.6865539\n",
      "Training the model... Step : 19\n",
      "Epoch 100 | Loss: 0.68239063\n",
      "Epoch 200 | Loss: 0.6822502\n",
      "Epoch 300 | Loss: 0.68494505\n",
      "Epoch 400 | Loss: 0.6820211\n",
      "Epoch 500 | Loss: 0.68260586\n",
      "Train accuracy: 53.05642\n",
      "[[28494  3177]\n",
      " [26334  5337]]\n",
      "Test accuracy: 55.00833\n",
      "[[5187 4780]\n",
      " [ 619 1414]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1 0 1 ... 1 0 1].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ef097dcc4636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0my_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"f1 score : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    275\u001b[0m     return _average_binary_score(\n\u001b[1;32m    276\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mnot_average_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1 0 1 ... 1 0 1].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "num_steps=20 #Change\n",
    "def accuracy(predictions, labels):\n",
    "  P=np.argmax(predictions,1) \n",
    "  Y=np.argmax(labels,1)\n",
    "  return (100.0 * np.sum(np.argmax(predictions,1) == np.argmax(labels,1))    / predictions.shape[0])\n",
    "def accuracy1(pred):\n",
    "        test_predict1 = np.argmax(pred,1)\n",
    "        return test_predict1\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    # Initialize variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    for step in range(num_steps):\n",
    "        # Training\n",
    "        print('Training the model... Step :',step)\n",
    "        for i in range(1, (epoch + 1)):\n",
    "            session.run(optimizer, feed_dict={X_data: X_train, y_target: y_train})\n",
    "            if i % interval == 0:\n",
    "                print('Epoch', i, '|', 'Loss:', session.run(loss, feed_dict={X_data: X_train, y_target: y_train}))\n",
    "    #print()\n",
    "    #for i in range(len(X_test)):\n",
    "        #print('Actual:', y_test[i], 'Predicted:', np.rint(session.run(final_output, feed_dict={X_data: [X_test[i]]})))\n",
    "    print(\"Train accuracy: {:.5f}\".format(accuracy(train_prediction.eval(), y_train)))\n",
    "    cm_tr = confusion_matrix_tp.eval()\n",
    "    print(cm_tr)\n",
    "    print(\"Test accuracy: {:.5f}\".format(accuracy(test_prediction.eval(), y_test)))\n",
    "    cm_te = confusion_matrix_tf.eval()\n",
    "    y_p=accuracy1(test_prediction.eval())\n",
    "    print(cm_te)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
